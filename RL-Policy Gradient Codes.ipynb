{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Table of Content  </h1>\n",
    "\n",
    "<ol>\n",
    "    <li><span style=\"color:green\">OpenAI-GYM setup : Pong and cartpool</span></li> \n",
    "    <li><span style=\"color:green\">Generate Experiece:</span></li>\n",
    "    <ul>\n",
    "        <li>Buffer to store trajectories</li>\n",
    "        <li>Reward Transformations: Advantage, GAE etc</li>\n",
    "        <li>Normalizing reward</li>\n",
    "        <li>Observations preprocessing methods</li>\n",
    "    </ul>\n",
    "    <li><span style=\"color:green\">Models code: Actor, critic etc</span></li>\n",
    "    <li>Training Codes</li>\n",
    "    <ul>\n",
    "        <li>Optimizers: LR, Initialization etc</li>\n",
    "        <li>Metrics To track: Reward, entropy , etc</li>\n",
    "        <li>Tensorboard</li>\n",
    "        <li>Logging</li>\n",
    "        <li>Model Checkpoints</li>\n",
    "        <li>Hyper-parameter search Techniques</li>\n",
    "        <li>Script to automate process</li>\n",
    "    </ul>\n",
    "    <li>Update Rules: Policy Algos</li>\n",
    "    <ul>\n",
    "        <li>Actor Update rule: Vanila, TPO, PPO</li>\n",
    "        <li>Value Funtion fit: Regression on discounted reward, TD(1)</li>\n",
    "    </ul>\n",
    "    <li>Documentation and Good Reference </li>\n",
    "</ol>    \n",
    "\n",
    "\n",
    "<p><b>Referencess ::</b>\n",
    "\n",
    "1. Spinningup openAI  2. Karapathy Pong from pixel   3. \n",
    "    \n",
    "A. https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
    "    \n",
    "   https://github.com/openai/spinningup/tree/038665d62d569055401d91856abb287263096178/spinup/algos/pytorch/vpg\n",
    "\n",
    "B. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5    \n",
    "</p>\n",
    "\n",
    "\n",
    "<b>Important Observations</b>\n",
    "\n",
    "- even for episodic task, use GAE or discounted reward for each observation leading upto terminal reward\n",
    "\n",
    "#Pong-v0\n",
    "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff\n",
    "\n",
    "\n",
    "- actions:: 0> no movement 2>>up 3>> down\n",
    "- Reward:: +1 -> win::  -1 -> loss :: 0 -> otherwise\n",
    "- [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0 …]\n",
    "This is not very useful, as we’ve no idea about the significance of the actions preceding the reward, and the number of actions preceding a reward may vary. To address this, we can introduce a discount function, which utilizes a decay rate (gamma, defined earlier) to distribute the normalized earned reward across a number of preceding frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "import gym\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes=[256], model = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "\n",
    "        if model == None:\n",
    "            # obsdim --> obs_dim --> 256 --> act_dim\n",
    "            ind = self.obs_dim\n",
    "            layers = []\n",
    "            for l in hidden_sizes:\n",
    "                layers.append(nn.Linear(ind,l))\n",
    "                layers.append(nn.ReLU())\n",
    "                ind = l\n",
    "\n",
    "            layers.append(nn.Linear(ind, self.act_dim))\n",
    "            model = nn.Sequential(*layers)\n",
    "\n",
    "        self.logits_net = model\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        logits = self.logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act)\n",
    "\n",
    "    def forward(self, obs, act=None):\n",
    "        # Produce action distributions for given observations, and \n",
    "        # optionally compute the log likelihood of given actions under\n",
    "        # those distributions.\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = pi.log_prob(act)\n",
    "        return pi, logp_a\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, hidden_sizes=[256], model = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if model == None:\n",
    "            ind = obs_dim\n",
    "            layers = []\n",
    "            for l in hidden_sizes:\n",
    "                layers.append(nn.Linear(ind,l))\n",
    "                layers.append(nn.ReLU())\n",
    "                ind = l\n",
    "\n",
    "            layers.append(nn.Linear(ind, 1))\n",
    "            model = nn.Sequential(*layers)                          \n",
    "        \n",
    "        self.v_net = model\n",
    "            \n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape.\n",
    "\n",
    "    \n",
    "\n",
    "class Agent(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Data collection\n",
    "    1.STEP:: obs --> step --> act, v, log\n",
    "    \n",
    "    Training\n",
    "    2. self.pi   obs --> pi --> prob/logp_a\n",
    "    3. Self.v    ovs --> v ---> v\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, \n",
    "                 hidden_sizes=[64,64], actor_model=None,critic_model=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.pi = Actor(obs_dim, act_dim, hidden_sizes, actor_model) \n",
    "\n",
    "        # build value function\n",
    "        self.v  = Critic(obs_dim, hidden_sizes, critic_model)\n",
    "\n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi._distribution(obs)\n",
    "            a = pi.sample()\n",
    "            logp_a = pi.log_prob(a)  #self.pi._log_prob_from_distribution(pi, a)\n",
    "            v = self.v(obs)\n",
    "        return a.cpu().numpy(), v.cpu().numpy(), logp_a.cpu().numpy()\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.step(obs)[0]    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pong\n",
    "\n",
    "\"\"\"\n",
    "action:: 2 --> up\n",
    "         3 --> down\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BufferNew:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def discount_rewards(self,r, gamma):\n",
    "        \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "        r = np.array(r)\n",
    "        discounted_r = np.zeros_like(r)\n",
    "        running_add = 0\n",
    "\n",
    "        for t in reversed(range(0, r.size)):\n",
    "            if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset \n",
    "            running_add = running_add * gamma + r[t] \n",
    "            discounted_r[t] = running_add\n",
    "        return discounted_r\n",
    "    \n",
    "    def normalize(self, r):\n",
    "        r = np.array(r)\n",
    "        r -= np.mean(r) #normalizing the result\n",
    "        r /= np.std(r) #idem using standar deviation\n",
    "        return r\n",
    "        \n",
    "    def reset(self, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = [] \n",
    "        self.act_buf = [] \n",
    "        self.rew_buf = [] \n",
    "        \n",
    "        self.val_buf = [] \n",
    "        self.logp_buf = []\n",
    "        \n",
    "        # Used for training\n",
    "        self.adv_buf = [] \n",
    "        self.ret_buf = []\n",
    "\n",
    "        \n",
    "        \n",
    "        #---------environmet----------------\n",
    "        self.obs_buf_ep = [] \n",
    "        self.act_buf_ep = [] \n",
    "        self.rew_buf_ep = [] \n",
    "        \n",
    "        #---------Model---------------------\n",
    "        self.val_buf_ep = [] \n",
    "        self.logp_buf_ep = []\n",
    "\n",
    "        \n",
    "        #-------For GAE training-------------\n",
    "        self.adv_buf_ep = [] \n",
    "        self.ret_buf_ep = [] \n",
    "        \n",
    "        self.gamma, self.lam = gamma, lam\n",
    "    \n",
    "    def __init__(self, gamma=0.99, lam=0.95, dev = torch.device('cpu')):\n",
    "        self.epochs = 0 \n",
    "        self.reset(gamma, lam)\n",
    "        self.device = dev\n",
    "        \n",
    "        \n",
    "\n",
    "    def store(self, obs =0, act = 0, rew = 0, val = 0, logp = 0):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "#         print(\"store called\")\n",
    "        self.obs_buf_ep.append(obs)\n",
    "        self.act_buf_ep.append(act)\n",
    "        self.rew_buf_ep.append(rew)\n",
    "        \n",
    "        self.val_buf_ep.append(val)\n",
    "        self.logp_buf_ep.append(logp)\n",
    "\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "        #print(\"inside finish_path\")\n",
    "        self.epochs += 1\n",
    "        rews = np.append( np.array(self.rew_buf_ep), last_val)\n",
    "        vals = np.append( np.array(self.val_buf_ep), last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf_ep = self.discount_rewards(deltas, self.gamma * self.lam)\n",
    "        self.adv_buf_ep = self.normalize(self.adv_buf_ep)\n",
    "#         print(\"advantage calculated\")\n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf_ep = self.discount_rewards(rews, self.gamma)[:-1]\n",
    "#         print(\"ret done\")\n",
    "        \n",
    "        #print(f\"rews : {rews.shape} \\n vals: {vals.shape} \\n adv_buf : {self.adv_buf_ep.shape} \\n ret: {self.ret_buf_ep.shape}\")\n",
    "\n",
    "#        print(\"path finished\")\n",
    "        \n",
    "#         if self.epochs % 100 == 0:\n",
    "#             x = np.arange(0,len(self.adv_buf_ep))\n",
    "#             plt.clf()\n",
    "#             plt.title(\"reward/advantage curve\")\n",
    "#             plt.plot(x, self.adv_buf_ep, label=\"advantage\")\n",
    "#             plt.plot(x, self.ret_buf_ep, label=\"rew to go\")\n",
    "#             plt.xlabel(\"samples\")\n",
    "#             plt.ylabel(\"adv/rewtogo\")\n",
    "#             plt.legend(loc='best')\n",
    "#             plt.savefig(os.path.join(os.getcwd(),\"Graph\", str(self.epochs)+\"_\"+\"adv_rew.png\"))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.obs_buf.append(self.obs_buf_ep) \n",
    "        self.act_buf.append(self.act_buf_ep) \n",
    "        self.rew_buf.append(self.rew_buf_ep)\n",
    "        \n",
    "        self.val_buf.append(self.val_buf_ep) \n",
    "        self.logp_buf.append(self.logp_buf_ep)\n",
    "        \n",
    "        # Used for training\n",
    "        self.adv_buf.append(self.adv_buf_ep) \n",
    "        self.ret_buf.append(self.ret_buf_ep)\n",
    "#         print(\"done all \")\n",
    "        #store episodes\n",
    "    \n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        #print(\"get() :: dictionary created\")\n",
    "        \n",
    "        data_t = {}\n",
    "        for k,v in data.items():\n",
    "            #print(k)\n",
    "            v = np.array(v)\n",
    "            #print(v.shape)\n",
    "            #print(v.dtype)\n",
    "            data_t[k] = torch.as_tensor(v, dtype=torch.float32).to(self.device)\n",
    "        return data_t#{k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing VPG policy loss\n",
    "def compute_loss_pi(data, ac):\n",
    "    obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "#     print(f\"{obs.device} {act.device}\")\n",
    "    # Policy loss\n",
    "    pi, logp = ac.pi(obs, act)\n",
    "    loss_pi = -(logp * adv).mean()\n",
    "    #print(f\"\\n compute_loss_pi:: \\n\\n obs: {obs.shape} \\n adv: {adv.shape}\\n logp {logp.shape}\\n loss_pi = -(logp * adv).mean() --> {loss_pi.shape} \")\n",
    "\n",
    "    # Useful extra info\n",
    "    approx_kl = (logp_old - logp).mean().item()\n",
    "    ent = pi.entropy().mean().item()\n",
    "    pi_info = dict(kl=approx_kl, ent=ent)\n",
    "\n",
    "    return loss_pi, pi_info\n",
    "\n",
    "# Set up function for computing value loss\n",
    "def compute_loss_v(data, ac):\n",
    "    obs, ret = data['obs'], data['ret']\n",
    "    l = ((ac.v(obs) - ret)**2).mean()\n",
    "    #print(f\"\\n compute_loss_v \\nobs: {obs.shape} \\n ret: {ret.shape} \\n ((ac.v(obs) - ret)**2).mean() : {l.shape} \")\n",
    "    return l\n",
    "\n",
    "def update(data, pi_optimizer, train_v_iters, vf_optimizer, ac):\n",
    "   \n",
    "    # Get loss and info values before update\n",
    "    pi_l_old, pi_info_old = compute_loss_pi(data, ac)\n",
    "    pi_l_old = pi_l_old.item()\n",
    "    v_l_old = compute_loss_v(data, ac).item()\n",
    "\n",
    "    # Train policy with a single step of gradient descent\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi, pi_info = compute_loss_pi(data, ac)\n",
    "    pi_loss = loss_pi.item()\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Value function learning\n",
    "    for i in range(train_v_iters):\n",
    "        vf_optimizer.zero_grad()\n",
    "        loss_v = compute_loss_v(data, ac)\n",
    "        loss_v.backward()\n",
    "        v_loss = loss_v.item()\n",
    "        vf_optimizer.step()\n",
    "\n",
    "    # Log changes from update\n",
    "    kl, ent = pi_info['kl'], pi_info_old['ent']\n",
    "    #print(f\"kl: {kl} \\n ent: {ent}\")\n",
    "    return pi_loss, v_loss, ent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Reinforcement\\PolicyGradient\\model\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%matplotlib inline  \n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('pg_800')\n",
    "# %tensorboard --logdir=runs/vpg_1\n",
    "# #model\n",
    "# writer.add_graph(net, images)\n",
    "# writer.close()\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "checkpt = os.path.join(cwd,\"model\")#,str(5))\n",
    "# checkpt += \".pt\"\n",
    "print(checkpt)\n",
    "\n",
    "\n",
    "\n",
    "epochs=10000\n",
    "gamma=0.99\n",
    "pi_lr=1e-4\n",
    "vf_lr=1e-4 \n",
    "train_v_iters=60\n",
    "lam=0.97\n",
    "max_ep_len=1000,\n",
    "save_freq=50\n",
    "log_freq = 10\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "last_epoch = 5450\n",
    "\n",
    "loadpth = os.path.join(checkpt,\"agent_\" + str(last_epoch) + \".pt\")\n",
    "\n",
    "# Create actor-critic module\n",
    "ac = Agent(80*80, 2, hidden_sizes = [256,64])\n",
    "ac.load_state_dict(torch.load(loadpth))\n",
    "ac.to(device)\n",
    "\n",
    "buf = BufferNew(gamma, lam, device)\n",
    "\n",
    "# Set up optimizers for policy and value function\n",
    "pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "vf_optimizer = Adam(ac.v.parameters(), lr=vf_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "logDict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n"
     ]
    }
   ],
   "source": [
    "print(len(logDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5460: rew_aw: -10.8 || len_Av: 5822.5 || pi_loss:-0.0013523441448342055 || v_loss:0.12162694707512856 || entropy: 0.06388965025544166\n",
      "5470: rew_aw: -14.3 || len_Av: 5571.4 || pi_loss:-0.0008889262782759034 || v_loss:0.10476735085248948 || entropy: 0.06247897632420063\n",
      "5480: rew_aw: -14.4 || len_Av: 5390.5 || pi_loss:0.0019264152884716167 || v_loss:0.09798915758728981 || entropy: 0.06134676150977612\n",
      "5490: rew_aw: -12.8 || len_Av: 5542.4 || pi_loss:-0.001411511363403406 || v_loss:0.1108429528772831 || entropy: 0.06101363413035869\n",
      "5500: rew_aw: -14.5 || len_Av: 5255.3 || pi_loss:-0.0009891746667562984 || v_loss:0.10279479995369911 || entropy: 0.061430885642766955\n",
      "5510: rew_aw: -12.7 || len_Av: 5864.4 || pi_loss:-0.0011533689219504594 || v_loss:0.1087005116045475 || entropy: 0.061104702204465865\n",
      "5520: rew_aw: -14.2 || len_Av: 5489.9 || pi_loss:0.0040759362513199445 || v_loss:0.10360387489199638 || entropy: 0.060480961576104164\n",
      "5530: rew_aw: -15.0 || len_Av: 4943.4 || pi_loss:0.00022453577839769422 || v_loss:0.10437087342143059 || entropy: 0.06008883155882359\n",
      "5540: rew_aw: -13.9 || len_Av: 5414.2 || pi_loss:-1.5976849135768133e-05 || v_loss:0.10101778879761696 || entropy: 0.060677656531333925\n",
      "5550: rew_aw: -14.1 || len_Av: 5227.6 || pi_loss:0.0003459782776189968 || v_loss:0.10169338919222355 || entropy: 0.06321966610848903\n",
      "5560: rew_aw: -14.9 || len_Av: 5340.4 || pi_loss:5.463554807647597e-05 || v_loss:0.10429444536566734 || entropy: 0.059110996499657634\n",
      "5570: rew_aw: -14.0 || len_Av: 5338.7 || pi_loss:-0.0006456277245888486 || v_loss:0.10883082225918769 || entropy: 0.05965346246957779\n",
      "5580: rew_aw: -15.3 || len_Av: 4729.7 || pi_loss:0.00042359888320788743 || v_loss:0.09619594961404801 || entropy: 0.058281199634075166\n",
      "5590: rew_aw: -14.4 || len_Av: 5654.6 || pi_loss:0.0005265223560854792 || v_loss:0.1012967050075531 || entropy: 0.05762915685772896\n",
      "5600: rew_aw: -14.1 || len_Av: 5509.4 || pi_loss:-0.00016400717431679367 || v_loss:0.10778601169586181 || entropy: 0.058854340389370915\n",
      "5610: rew_aw: -13.1 || len_Av: 5564.6 || pi_loss:0.001257116167107597 || v_loss:0.1099173367023468 || entropy: 0.05998050719499588\n",
      "5620: rew_aw: -13.1 || len_Av: 5675.3 || pi_loss:0.0002904796026996337 || v_loss:0.09861582592129707 || entropy: 0.06169324554502964\n",
      "5630: rew_aw: -15.4 || len_Av: 5247.4 || pi_loss:0.0005670192651450634 || v_loss:0.09636342823505402 || entropy: 0.06118204109370708\n",
      "5640: rew_aw: -13.1 || len_Av: 5446.0 || pi_loss:-5.0565877245389856e-05 || v_loss:0.10712131857872009 || entropy: 0.061149609833955766\n",
      "5650: rew_aw: -13.2 || len_Av: 5489.3 || pi_loss:-0.0007739218301139772 || v_loss:0.1135530300438404 || entropy: 0.06085768975317478\n",
      "5660: rew_aw: -12.8 || len_Av: 5622.3 || pi_loss:-0.0016755979479057715 || v_loss:0.11303118094801903 || entropy: 0.06175885386765003\n",
      "5670: rew_aw: -14.3 || len_Av: 5399.6 || pi_loss:0.0011747828335501253 || v_loss:0.10199925899505616 || entropy: 0.06037782244384289\n",
      "5680: rew_aw: -14.7 || len_Av: 5536.2 || pi_loss:0.0014743107865797355 || v_loss:0.09799308031797409 || entropy: 0.059513794258236885\n",
      "5690: rew_aw: -13.8 || len_Av: 5548.4 || pi_loss:0.0007822662693797611 || v_loss:0.11332391500473023 || entropy: 0.0580995500087738\n",
      "5700: rew_aw: -13.9 || len_Av: 5387.3 || pi_loss:0.0008354505676834379 || v_loss:0.09933552369475365 || entropy: 0.059485217556357384\n",
      "5710: rew_aw: -14.1 || len_Av: 5480.2 || pi_loss:0.0025607612493331543 || v_loss:0.10719340369105339 || entropy: 0.059056033939123155\n",
      "5720: rew_aw: -12.6 || len_Av: 5339.4 || pi_loss:4.0035764686763284e-05 || v_loss:0.11509700566530227 || entropy: 0.06051548980176449\n",
      "5730: rew_aw: -13.0 || len_Av: 5721.9 || pi_loss:0.0002485786306351656 || v_loss:0.10063277892768382 || entropy: 0.059830525889992714\n",
      "5740: rew_aw: -14.2 || len_Av: 5112.5 || pi_loss:0.0007417958480800735 || v_loss:0.10987793356180191 || entropy: 0.06034843809902668\n",
      "5750: rew_aw: -11.9 || len_Av: 5335.6 || pi_loss:-0.0008085047244094312 || v_loss:0.11149010062217712 || entropy: 0.057884260267019275\n",
      "5760: rew_aw: -13.3 || len_Av: 5631.3 || pi_loss:-0.0012996918085264042 || v_loss:0.10942282006144524 || entropy: 0.059853047132492065\n",
      "5770: rew_aw: -15.1 || len_Av: 5446.2 || pi_loss:0.0002203949959948659 || v_loss:0.10233035534620286 || entropy: 0.05831942521035671\n",
      "5780: rew_aw: -15.2 || len_Av: 5274.0 || pi_loss:0.0011188289034180343 || v_loss:0.09730650037527085 || entropy: 0.05894730538129807\n",
      "5790: rew_aw: -13.1 || len_Av: 5726.7 || pi_loss:-0.00013156673521734773 || v_loss:0.10637072995305061 || entropy: 0.05997328795492649\n",
      "5800: rew_aw: -14.8 || len_Av: 5194.9 || pi_loss:0.0022824636194854974 || v_loss:0.09860917702317237 || entropy: 0.059727951511740686\n",
      "5810: rew_aw: -13.7 || len_Av: 5296.9 || pi_loss:0.0033549066152772865 || v_loss:0.11239067539572715 || entropy: 0.061051464453339574\n",
      "5820: rew_aw: -12.8 || len_Av: 5900.4 || pi_loss:-0.0003514337164233439 || v_loss:0.10877949818968773 || entropy: 0.06177981235086918\n",
      "5830: rew_aw: -14.9 || len_Av: 5307.2 || pi_loss:-0.000992575319833122 || v_loss:0.09764303490519524 || entropy: 0.0629049975425005\n",
      "5840: rew_aw: -14.4 || len_Av: 5438.1 || pi_loss:0.0029917222735093675 || v_loss:0.10152853876352311 || entropy: 0.06021330840885639\n",
      "5850: rew_aw: -12.8 || len_Av: 5518.1 || pi_loss:2.1543982438743114e-05 || v_loss:0.11386172994971275 || entropy: 0.06219024918973446\n",
      "5860: rew_aw: -14.1 || len_Av: 5398.6 || pi_loss:0.0001658779408899136 || v_loss:0.10897791758179665 || entropy: 0.05931034609675408\n",
      "5870: rew_aw: -13.3 || len_Av: 5364.7 || pi_loss:0.002917333575896919 || v_loss:0.11022162437438965 || entropy: 0.061483100056648254\n",
      "5880: rew_aw: -12.2 || len_Av: 5819.3 || pi_loss:0.0013848701026290655 || v_loss:0.11148552373051643 || entropy: 0.061623314023017885\n",
      "5890: rew_aw: -13.0 || len_Av: 5688.8 || pi_loss:0.0006650626368355006 || v_loss:0.10909315347671508 || entropy: 0.061893108487129214\n",
      "5900: rew_aw: -13.4 || len_Av: 5508.5 || pi_loss:-6.632904551224783e-05 || v_loss:0.10043964758515359 || entropy: 0.06113821938633919\n",
      "5910: rew_aw: -14.2 || len_Av: 5334.3 || pi_loss:0.0012411026167683304 || v_loss:0.1039639376103878 || entropy: 0.06239335685968399\n",
      "5920: rew_aw: -12.4 || len_Av: 6224.4 || pi_loss:0.0007546163524239091 || v_loss:0.10595297738909722 || entropy: 0.06062167510390282\n",
      "5930: rew_aw: -13.6 || len_Av: 5542.2 || pi_loss:-0.0014083710757404333 || v_loss:0.09874089136719703 || entropy: 0.06291999854147434\n",
      "5940: rew_aw: -14.6 || len_Av: 5324.3 || pi_loss:-0.00042826131539186463 || v_loss:0.09926716238260269 || entropy: 0.06120848916471004\n",
      "5950: rew_aw: -13.8 || len_Av: 5607.3 || pi_loss:0.001666629852843471 || v_loss:0.11218934580683708 || entropy: 0.06217354498803616\n",
      "5960: rew_aw: -16.1 || len_Av: 5089.1 || pi_loss:-0.00016744319000281394 || v_loss:0.09403134547173977 || entropy: 0.06187625899910927\n",
      "5970: rew_aw: -14.3 || len_Av: 5541.9 || pi_loss:-0.0004252823656315741 || v_loss:0.09960136637091636 || entropy: 0.06347829215228558\n",
      "5980: rew_aw: -13.8 || len_Av: 5252.4 || pi_loss:-0.0005152431724127382 || v_loss:0.10417983271181583 || entropy: 0.06113121397793293\n",
      "5990: rew_aw: -13.3 || len_Av: 6033.1 || pi_loss:-6.712531903758645e-05 || v_loss:0.10195568799972535 || entropy: 0.061832484975457194\n",
      "6000: rew_aw: -14.7 || len_Av: 5290.1 || pi_loss:-0.0002141832374036312 || v_loss:0.10251143723726272 || entropy: 0.06130911372601986\n",
      "6010: rew_aw: -14.0 || len_Av: 5399.2 || pi_loss:-0.0018948361685033888 || v_loss:0.10227625891566276 || entropy: 0.062996531650424\n",
      "6020: rew_aw: -13.3 || len_Av: 5397.7 || pi_loss:-0.0012847997801145538 || v_loss:0.10364466309547424 || entropy: 0.06304064430296422\n",
      "6030: rew_aw: -14.0 || len_Av: 5440.6 || pi_loss:0.0009820058301556855 || v_loss:0.10486653074622154 || entropy: 0.06509398594498635\n",
      "6040: rew_aw: -14.3 || len_Av: 5193.5 || pi_loss:-0.00030189420795068145 || v_loss:0.10211087316274643 || entropy: 0.06600552424788475\n",
      "6050: rew_aw: -14.2 || len_Av: 5648.3 || pi_loss:-0.0018783069914206862 || v_loss:0.10023236721754074 || entropy: 0.06707526966929436\n",
      "6060: rew_aw: -12.4 || len_Av: 5916.9 || pi_loss:-0.0029052136233076453 || v_loss:0.1128067821264267 || entropy: 0.06393205635249614\n",
      "6070: rew_aw: -14.4 || len_Av: 5390.2 || pi_loss:-0.0009094253007788211 || v_loss:0.0991608090698719 || entropy: 0.06294076591730118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6080: rew_aw: -15.2 || len_Av: 5038.4 || pi_loss:0.0014235732975066639 || v_loss:0.10886259153485298 || entropy: 0.06478055976331235\n",
      "6090: rew_aw: -14.7 || len_Av: 5284.2 || pi_loss:0.0007753332596621476 || v_loss:0.1064748503267765 || entropy: 0.061037341877818105\n",
      "6100: rew_aw: -13.8 || len_Av: 5672.3 || pi_loss:0.0007935044646728784 || v_loss:0.10678702890872956 || entropy: 0.0632092732936144\n",
      "6110: rew_aw: -13.7 || len_Av: 5430.6 || pi_loss:0.0010873624618398026 || v_loss:0.10592545792460442 || entropy: 0.0642358459532261\n",
      "6120: rew_aw: -15.8 || len_Av: 4990.8 || pi_loss:0.0017842415254563093 || v_loss:0.09516115821897983 || entropy: 0.06389915123581887\n",
      "6130: rew_aw: -13.9 || len_Av: 5715.4 || pi_loss:2.5905435904860497e-05 || v_loss:0.106280467659235 || entropy: 0.06771675273776054\n",
      "6140: rew_aw: -13.2 || len_Av: 5548.2 || pi_loss:-0.001507756661158055 || v_loss:0.11112130507826805 || entropy: 0.06665247604250908\n",
      "6150: rew_aw: -15.0 || len_Av: 5250.9 || pi_loss:-0.00032100065727718173 || v_loss:0.10482095777988434 || entropy: 0.06461541205644608\n",
      "6160: rew_aw: -13.7 || len_Av: 5865.6 || pi_loss:-0.0001946745629538782 || v_loss:0.10354973673820496 || entropy: 0.06569225303828716\n",
      "6170: rew_aw: -13.7 || len_Av: 5613.5 || pi_loss:-0.00022921558920643292 || v_loss:0.10513588637113572 || entropy: 0.06385017149150371\n",
      "6180: rew_aw: -13.5 || len_Av: 4923.7 || pi_loss:0.0013879235892090946 || v_loss:0.11963516101241112 || entropy: 0.06463300511240959\n",
      "6190: rew_aw: -13.2 || len_Av: 5459.2 || pi_loss:-0.0005882351339096204 || v_loss:0.11251187324523926 || entropy: 0.06371705830097199\n",
      "6200: rew_aw: -12.2 || len_Av: 5510.7 || pi_loss:-0.00031156406766967847 || v_loss:0.11745910197496415 || entropy: 0.06418906375765801\n",
      "6210: rew_aw: -14.6 || len_Av: 5309.6 || pi_loss:-0.00182255728286691 || v_loss:0.10684615224599839 || entropy: 0.06506997607648372\n",
      "6220: rew_aw: -15.0 || len_Av: 4946.9 || pi_loss:6.853511149529368e-05 || v_loss:0.1143381081521511 || entropy: 0.06388753950595856\n",
      "6230: rew_aw: -14.3 || len_Av: 5261.2 || pi_loss:-0.0029885621428547894 || v_loss:0.10842429101467133 || entropy: 0.0616253886371851\n",
      "6240: rew_aw: -14.2 || len_Av: 5366.5 || pi_loss:-0.0018876506830565632 || v_loss:0.11102326884865761 || entropy: 0.064557184278965\n",
      "6250: rew_aw: -15.8 || len_Av: 4813.6 || pi_loss:0.00034981666249223053 || v_loss:0.10929628014564514 || entropy: 0.06156322248280048\n",
      "6260: rew_aw: -14.7 || len_Av: 5231.0 || pi_loss:0.00026538092643022535 || v_loss:0.1035069540143013 || entropy: 0.0615133136510849\n",
      "6270: rew_aw: -12.8 || len_Av: 5952.8 || pi_loss:0.0007541627390310168 || v_loss:0.10591369718313218 || entropy: 0.06187464855611324\n",
      "6280: rew_aw: -12.6 || len_Av: 5819.4 || pi_loss:0.0002319411898497492 || v_loss:0.10550636723637581 || entropy: 0.06001567766070366\n",
      "6290: rew_aw: -12.7 || len_Av: 5525.6 || pi_loss:-0.0019792301231063903 || v_loss:0.11385628208518028 || entropy: 0.061883645877242086\n",
      "6300: rew_aw: -14.9 || len_Av: 5384.9 || pi_loss:0.00015590721959597432 || v_loss:0.10429176464676856 || entropy: 0.061805590987205505\n",
      "6310: rew_aw: -13.5 || len_Av: 5561.1 || pi_loss:-0.0002606459194794297 || v_loss:0.1096215859055519 || entropy: 0.06340457499027252\n",
      "6320: rew_aw: -13.5 || len_Av: 5429.5 || pi_loss:0.00017934958450496196 || v_loss:0.10731492266058922 || entropy: 0.062292524799704554\n",
      "6330: rew_aw: -11.1 || len_Av: 5995.4 || pi_loss:0.00201036112298425 || v_loss:0.11727381572127342 || entropy: 0.06174434944987297\n",
      "6340: rew_aw: -13.0 || len_Av: 5539.8 || pi_loss:-0.0014328294913866558 || v_loss:0.11698202565312385 || entropy: 0.06195701733231544\n",
      "6350: rew_aw: -13.2 || len_Av: 5381.6 || pi_loss:-0.0027797958289738746 || v_loss:0.10934529155492782 || entropy: 0.06322194300591946\n",
      "6360: rew_aw: -15.5 || len_Av: 5138.3 || pi_loss:-0.0013276793586555868 || v_loss:0.0892032764852047 || entropy: 0.061868470162153244\n",
      "6370: rew_aw: -13.7 || len_Av: 5266.2 || pi_loss:-0.001407193738850765 || v_loss:0.11052684709429741 || entropy: 0.060778426378965376\n",
      "6380: rew_aw: -13.1 || len_Av: 5687.6 || pi_loss:0.0003402363352734028 || v_loss:0.10702694207429886 || entropy: 0.06077773943543434\n",
      "6390: rew_aw: -12.2 || len_Av: 5865.0 || pi_loss:0.0018569319538073615 || v_loss:0.10639853179454803 || entropy: 0.060766198113560675\n",
      "6400: rew_aw: -13.5 || len_Av: 5306.0 || pi_loss:0.0029234120673208965 || v_loss:0.10935089588165284 || entropy: 0.06174207516014576\n",
      "6410: rew_aw: -13.1 || len_Av: 5932.8 || pi_loss:0.0005208853399381042 || v_loss:0.10194919779896736 || entropy: 0.06014354787766933\n",
      "6420: rew_aw: -13.8 || len_Av: 5468.5 || pi_loss:-0.0002852932342648273 || v_loss:0.1073378086090088 || entropy: 0.06028165221214295\n",
      "6430: rew_aw: -15.1 || len_Av: 4921.3 || pi_loss:-0.0018236904128571042 || v_loss:0.10716504380106925 || entropy: 0.05787852555513382\n",
      "6440: rew_aw: -13.4 || len_Av: 5664.4 || pi_loss:0.0004172241722699255 || v_loss:0.10709684416651725 || entropy: 0.0591523215174675\n",
      "6450: rew_aw: -15.1 || len_Av: 5185.4 || pi_loss:-0.0028022442158544434 || v_loss:0.10357582122087479 || entropy: 0.05846757665276527\n",
      "6460: rew_aw: -13.5 || len_Av: 5622.7 || pi_loss:-0.0009611287858206196 || v_loss:0.10959495157003403 || entropy: 0.057837804406881334\n",
      "6470: rew_aw: -14.5 || len_Av: 5599.8 || pi_loss:-3.206891560694203e-05 || v_loss:0.10343171581625939 || entropy: 0.05847203396260738\n",
      "6480: rew_aw: -13.2 || len_Av: 5273.0 || pi_loss:0.001404677367190743 || v_loss:0.11563176438212394 || entropy: 0.0569592509418726\n",
      "6490: rew_aw: -13.5 || len_Av: 5881.8 || pi_loss:-0.0005062532938609366 || v_loss:0.11032439321279526 || entropy: 0.05735665895044804\n",
      "6500: rew_aw: -14.5 || len_Av: 5488.4 || pi_loss:0.00019350596703588964 || v_loss:0.10587119311094284 || entropy: 0.05634361319243908\n",
      "6510: rew_aw: -13.2 || len_Av: 5841.9 || pi_loss:0.0004582876790664159 || v_loss:0.11050097197294236 || entropy: 0.05690411627292633\n",
      "6520: rew_aw: -13.4 || len_Av: 5578.2 || pi_loss:0.0015132157772313804 || v_loss:0.10972593277692795 || entropy: 0.056385836005210875\n",
      "6530: rew_aw: -14.3 || len_Av: 4957.4 || pi_loss:0.00026626415783539417 || v_loss:0.09942652881145478 || entropy: 0.056719638779759406\n",
      "6540: rew_aw: -11.5 || len_Av: 5703.2 || pi_loss:0.0007430705707520246 || v_loss:0.1148752972483635 || entropy: 0.05906657911837101\n",
      "6550: rew_aw: -13.7 || len_Av: 5692.1 || pi_loss:-0.002189417963381857 || v_loss:0.10348087400197983 || entropy: 0.05976427495479584\n",
      "6560: rew_aw: -16.2 || len_Av: 5101.6 || pi_loss:-0.002586385724134743 || v_loss:0.0928256280720234 || entropy: 0.06208331175148487\n",
      "6570: rew_aw: -15.6 || len_Av: 5339.4 || pi_loss:-0.0003873138433846179 || v_loss:0.1005195952951908 || entropy: 0.061780811101198194\n",
      "6580: rew_aw: -12.8 || len_Av: 5694.5 || pi_loss:6.28664216492325e-05 || v_loss:0.10681993961334228 || entropy: 0.0631689302623272\n",
      "6590: rew_aw: -12.3 || len_Av: 5307.2 || pi_loss:-0.0026082533440785483 || v_loss:0.12143412083387375 || entropy: 0.062177132442593576\n",
      "6600: rew_aw: -14.6 || len_Av: 5012.9 || pi_loss:0.0024805477529298513 || v_loss:0.11608614698052407 || entropy: 0.06209978237748146\n",
      "6610: rew_aw: -15.9 || len_Av: 4936.1 || pi_loss:-0.0006576188170583919 || v_loss:0.09158405512571335 || entropy: 0.06169275976717472\n",
      "6620: rew_aw: -15.7 || len_Av: 4759.5 || pi_loss:-0.000836809235624969 || v_loss:0.10240037962794304 || entropy: 0.061671844124794005\n",
      "6630: rew_aw: -13.9 || len_Av: 5006.1 || pi_loss:-0.0022746440954506397 || v_loss:0.10301704928278924 || entropy: 0.062409340590238574\n",
      "6640: rew_aw: -12.7 || len_Av: 5410.1 || pi_loss:0.0023283134307348517 || v_loss:0.11954566761851311 || entropy: 0.06031346432864666\n",
      "6650: rew_aw: -14.8 || len_Av: 5103.1 || pi_loss:0.0010494598420336843 || v_loss:0.10368493720889091 || entropy: 0.058188581466674806\n",
      "6660: rew_aw: -12.7 || len_Av: 6007.0 || pi_loss:-0.001827292880625464 || v_loss:0.10989924818277359 || entropy: 0.06026307865977287\n",
      "6670: rew_aw: -14.4 || len_Av: 5472.0 || pi_loss:0.0011303772858809679 || v_loss:0.10081088691949844 || entropy: 0.06178080253303051\n",
      "6680: rew_aw: -14.5 || len_Av: 5247.1 || pi_loss:0.000679457190562971 || v_loss:0.09847903326153755 || entropy: 0.05964022204279899\n",
      "6690: rew_aw: -13.2 || len_Av: 5776.9 || pi_loss:-0.001447037658363115 || v_loss:0.09769336953759193 || entropy: 0.06059048920869827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6700: rew_aw: -15.5 || len_Av: 5460.0 || pi_loss:0.0015312048664782197 || v_loss:0.09818611741065979 || entropy: 0.05922613367438316\n",
      "6710: rew_aw: -14.9 || len_Av: 5142.6 || pi_loss:-8.655198034830391e-05 || v_loss:0.10708827003836632 || entropy: 0.06046402081847191\n",
      "6720: rew_aw: -13.5 || len_Av: 5378.1 || pi_loss:0.0008051005366723985 || v_loss:0.10888257399201393 || entropy: 0.05996761023998261\n",
      "6730: rew_aw: -14.7 || len_Av: 5239.2 || pi_loss:-0.0006084573513362556 || v_loss:0.10091160088777543 || entropy: 0.0590776152908802\n",
      "6740: rew_aw: -12.7 || len_Av: 5574.3 || pi_loss:0.0016791721514891832 || v_loss:0.11300623789429665 || entropy: 0.05801100879907608\n",
      "6750: rew_aw: -14.5 || len_Av: 5500.5 || pi_loss:0.0006515280016174074 || v_loss:0.10656441077589988 || entropy: 0.057997891679406166\n",
      "6760: rew_aw: -13.9 || len_Av: 5278.4 || pi_loss:-0.0005196622398216278 || v_loss:0.10489431321620941 || entropy: 0.057822690531611445\n",
      "6770: rew_aw: -13.9 || len_Av: 5107.0 || pi_loss:-0.00017034416378010065 || v_loss:0.11823732107877731 || entropy: 0.05753505639731884\n",
      "6780: rew_aw: -15.6 || len_Av: 5061.3 || pi_loss:0.001449110804242082 || v_loss:0.10381248965859413 || entropy: 0.058335139974951745\n",
      "6790: rew_aw: -12.9 || len_Av: 5688.7 || pi_loss:0.0010175666973736952 || v_loss:0.10835459381341934 || entropy: 0.05973251536488533\n",
      "6800: rew_aw: -12.3 || len_Av: 5575.6 || pi_loss:0.0019874408026225865 || v_loss:0.11689754277467727 || entropy: 0.06009759530425072\n",
      "6810: rew_aw: -15.1 || len_Av: 5043.3 || pi_loss:-0.0023922556138131768 || v_loss:0.10695662200450898 || entropy: 0.05784608349204064\n",
      "6820: rew_aw: -12.5 || len_Av: 5573.8 || pi_loss:6.481625023297965e-05 || v_loss:0.11427628621459007 || entropy: 0.059311174973845485\n",
      "6830: rew_aw: -11.5 || len_Av: 5449.4 || pi_loss:-0.0014841612661257385 || v_loss:0.1142946258187294 || entropy: 0.058448294550180434\n",
      "6840: rew_aw: -13.1 || len_Av: 5618.9 || pi_loss:-0.0016591647353379813 || v_loss:0.11094715669751168 || entropy: 0.05697382800281048\n",
      "6850: rew_aw: -13.4 || len_Av: 5879.7 || pi_loss:-0.0025132557049801106 || v_loss:0.10406601056456566 || entropy: 0.057817130163311956\n",
      "6860: rew_aw: -14.0 || len_Av: 5981.6 || pi_loss:0.0005813182855490595 || v_loss:0.10109461098909378 || entropy: 0.05906539261341095\n",
      "6870: rew_aw: -13.4 || len_Av: 5621.7 || pi_loss:0.0012054576945956796 || v_loss:0.1043640747666359 || entropy: 0.05711212418973446\n",
      "6880: rew_aw: -14.2 || len_Av: 5697.5 || pi_loss:-0.0008419214456807822 || v_loss:0.10454136058688164 || entropy: 0.05846082903444767\n",
      "6890: rew_aw: -12.2 || len_Av: 5883.6 || pi_loss:-0.0023039993073325606 || v_loss:0.11288751289248466 || entropy: 0.058991075679659846\n",
      "6900: rew_aw: -13.4 || len_Av: 5651.8 || pi_loss:-0.0005948639882262796 || v_loss:0.10350459069013596 || entropy: 0.05893879383802414\n",
      "6910: rew_aw: -13.3 || len_Av: 5394.3 || pi_loss:-0.001479961366567295 || v_loss:0.11383806616067886 || entropy: 0.05864057466387749\n",
      "6920: rew_aw: -14.5 || len_Av: 5366.9 || pi_loss:-0.0027271427970845252 || v_loss:0.10516905710101128 || entropy: 0.057057547941803934\n",
      "6930: rew_aw: -14.3 || len_Av: 5297.2 || pi_loss:0.0015736089670099318 || v_loss:0.10505104809999466 || entropy: 0.057691114395856856\n",
      "6940: rew_aw: -10.3 || len_Av: 5885.4 || pi_loss:0.0017690863634925336 || v_loss:0.11790469661355019 || entropy: 0.058756885677576066\n",
      "6950: rew_aw: -13.9 || len_Av: 5764.3 || pi_loss:-0.002710839227074757 || v_loss:0.10238925516605377 || entropy: 0.056723298504948616\n",
      "6960: rew_aw: -14.4 || len_Av: 5392.3 || pi_loss:-0.0004551457444904372 || v_loss:0.10956503823399544 || entropy: 0.058356641605496404\n",
      "6970: rew_aw: -11.5 || len_Av: 5798.1 || pi_loss:-0.00010671278578229248 || v_loss:0.11843262538313866 || entropy: 0.059920713678002356\n",
      "6980: rew_aw: -14.2 || len_Av: 5341.7 || pi_loss:-0.00034557183971628547 || v_loss:0.09896150976419449 || entropy: 0.05842578075826168\n",
      "6990: rew_aw: -15.1 || len_Av: 5090.4 || pi_loss:-0.000553376224263502 || v_loss:0.10884097814559937 || entropy: 0.057888032868504524\n",
      "7000: rew_aw: -13.0 || len_Av: 5652.4 || pi_loss:-0.0016087948402855544 || v_loss:0.11619396209716797 || entropy: 0.058944734185934065\n",
      "7010: rew_aw: -14.4 || len_Av: 5599.4 || pi_loss:0.0016418898245319723 || v_loss:0.10322141796350479 || entropy: 0.0599657166749239\n",
      "7020: rew_aw: -15.3 || len_Av: 5133.1 || pi_loss:0.0004468178478191476 || v_loss:0.10057173222303391 || entropy: 0.05708259157836437\n",
      "7030: rew_aw: -13.7 || len_Av: 5383.4 || pi_loss:0.0018047282355837524 || v_loss:0.10699948593974114 || entropy: 0.05938179343938828\n",
      "7040: rew_aw: -13.8 || len_Av: 5183.3 || pi_loss:0.0003167514078086242 || v_loss:0.1097610630095005 || entropy: 0.058527587726712224\n",
      "7050: rew_aw: -15.2 || len_Av: 5207.5 || pi_loss:0.0038609347568126394 || v_loss:0.10832522064447403 || entropy: 0.05851528011262417\n",
      "7060: rew_aw: -13.9 || len_Av: 5658.3 || pi_loss:0.0007441944442689418 || v_loss:0.1089367114007473 || entropy: 0.06017970070242882\n",
      "7070: rew_aw: -13.1 || len_Av: 5309.9 || pi_loss:-0.0009391959349159152 || v_loss:0.11594868972897529 || entropy: 0.060905936360359195\n",
      "7080: rew_aw: -14.2 || len_Av: 5754.6 || pi_loss:-0.0010367732495069504 || v_loss:0.10176519975066185 || entropy: 0.0588001012802124\n",
      "7090: rew_aw: -14.8 || len_Av: 5811.8 || pi_loss:8.894987113308161e-05 || v_loss:0.10832252204418183 || entropy: 0.0588873665779829\n",
      "7100: rew_aw: -12.9 || len_Av: 5484.9 || pi_loss:-1.3652054622070865e-05 || v_loss:0.10917322933673859 || entropy: 0.05932639203965664\n",
      "7110: rew_aw: -13.2 || len_Av: 5903.4 || pi_loss:0.0016165287641342729 || v_loss:0.1092331774532795 || entropy: 0.060477476194500925\n",
      "7120: rew_aw: -13.4 || len_Av: 6077.4 || pi_loss:0.0013282583735417576 || v_loss:0.10733470991253853 || entropy: 0.06253127455711364\n",
      "7130: rew_aw: -11.5 || len_Av: 6072.6 || pi_loss:-0.001654142513871193 || v_loss:0.11305792555212975 || entropy: 0.0586544118821621\n",
      "7140: rew_aw: -14.8 || len_Av: 5223.6 || pi_loss:-0.001566294307122007 || v_loss:0.1037483423948288 || entropy: 0.057576443627476694\n",
      "7150: rew_aw: -14.0 || len_Av: 5504.6 || pi_loss:0.003085351585468743 || v_loss:0.10775776356458663 || entropy: 0.05872730016708374\n",
      "7160: rew_aw: -13.5 || len_Av: 5719.1 || pi_loss:-0.0018719059928116622 || v_loss:0.10791641846299171 || entropy: 0.06050117500126362\n",
      "7170: rew_aw: -12.7 || len_Av: 5896.6 || pi_loss:0.00023195564281195403 || v_loss:0.11052892059087753 || entropy: 0.058250050991773605\n",
      "7180: rew_aw: -13.1 || len_Av: 5910.4 || pi_loss:-0.000553408678388223 || v_loss:0.1060875654220581 || entropy: 0.058911771327257154\n",
      "7190: rew_aw: -13.3 || len_Av: 5549.7 || pi_loss:0.001289339237700915 || v_loss:0.09950556457042695 || entropy: 0.05728047862648964\n",
      "7200: rew_aw: -12.6 || len_Av: 5842.7 || pi_loss:0.0009424850635696202 || v_loss:0.10960932821035385 || entropy: 0.05542326755821705\n",
      "7210: rew_aw: -12.9 || len_Av: 5519.5 || pi_loss:0.0003373783052666113 || v_loss:0.1144973136484623 || entropy: 0.05663437880575657\n",
      "7220: rew_aw: -13.6 || len_Av: 5677.4 || pi_loss:0.0020482793799601494 || v_loss:0.10194945260882378 || entropy: 0.05613880343735218\n",
      "7230: rew_aw: -12.4 || len_Av: 6129.4 || pi_loss:0.0008232700783992186 || v_loss:0.10842976495623588 || entropy: 0.057849539816379546\n",
      "7240: rew_aw: -13.2 || len_Av: 6219.5 || pi_loss:0.0004971466958522796 || v_loss:0.09881217181682586 || entropy: 0.05695927180349827\n",
      "7250: rew_aw: -13.3 || len_Av: 6055.9 || pi_loss:0.0017175757995573804 || v_loss:0.10406965166330337 || entropy: 0.058508817851543424\n",
      "7260: rew_aw: -15.2 || len_Av: 5160.0 || pi_loss:0.003055989823769778 || v_loss:0.09249761551618577 || entropy: 0.057905062660574914\n",
      "7270: rew_aw: -13.8 || len_Av: 5894.8 || pi_loss:0.00031548717815894635 || v_loss:0.09723278135061264 || entropy: 0.05879416167736053\n",
      "7280: rew_aw: -14.3 || len_Av: 5419.7 || pi_loss:0.0002431043772958219 || v_loss:0.10582334324717521 || entropy: 0.05840121656656265\n",
      "7290: rew_aw: -14.5 || len_Av: 5622.3 || pi_loss:-0.00038250728757702743 || v_loss:0.09868109412491322 || entropy: 0.05872051306068897\n",
      "7300: rew_aw: -13.9 || len_Av: 5570.4 || pi_loss:0.0012181166891423346 || v_loss:0.10547379702329636 || entropy: 0.057348315790295604\n",
      "7310: rew_aw: -13.6 || len_Av: 5437.4 || pi_loss:0.0033872418280225247 || v_loss:0.11115349903702736 || entropy: 0.0589678443968296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7320: rew_aw: -15.1 || len_Av: 5542.8 || pi_loss:-0.0005178372128284536 || v_loss:0.1058199681341648 || entropy: 0.0586431585252285\n",
      "7330: rew_aw: -13.7 || len_Av: 5894.1 || pi_loss:-0.0003241919432184659 || v_loss:0.10835234969854354 || entropy: 0.05789959952235222\n",
      "7340: rew_aw: -13.8 || len_Av: 6006.4 || pi_loss:-0.0006266797165153549 || v_loss:0.0978843092918396 || entropy: 0.05772297009825707\n",
      "7350: rew_aw: -15.4 || len_Av: 5385.5 || pi_loss:0.0006230512633919715 || v_loss:0.11130209043622016 || entropy: 0.05508105270564556\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-283-fd005001f089>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mnext_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mep_ret\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mep_len\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Prepare for interaction with environment\n",
    "start_time = time.time()\n",
    "env = gym.make(\"Pong-v0\") \n",
    "o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "epr,epl = 0,0\n",
    "pilrun,vlrun, entrun = 0,0,0\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "for epoch in range(last_epoch+1,epochs):\n",
    "    buf.reset()\n",
    "    while True:\n",
    "        o = prepro(o)\n",
    "        a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32).to(device))\n",
    "        if a==0:\n",
    "            act = 2\n",
    "        else:\n",
    "            act = 3\n",
    "        next_o, r, d, _ = env.step(act)\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "        #print(ep_len)\n",
    "        # save and log\n",
    "        buf.store(o, a, r, v, logp)\n",
    "        #logger.store(VVals=v)\n",
    "\n",
    "        # Update obs (critical!)\n",
    "        o = next_o\n",
    "        \n",
    "        if d:\n",
    "            buf.finish_path(0)\n",
    "            epr += ep_ret\n",
    "            epl += ep_len\n",
    "            #print(f\"completed one game  r:{ep_ret} in {ep_len}\")\n",
    "            o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "            break\n",
    "            \n",
    "#         end = d or ep_len>1000\n",
    "#         if end:\n",
    "#             #print(\"done*****************\\n****************\\n***********\")\n",
    "#             if d:\n",
    "#                 buf.finish_path(0)\n",
    "#                 print(ep_ret)\n",
    "#                 o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "#                 break\n",
    "#             else:\n",
    "#                 val = ac.v(o)\n",
    "#                 buf.finish_path(val)\n",
    "#                 print(ep_ret)\n",
    "#                 o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "#                 break\n",
    "                \n",
    "\n",
    "  \n",
    "\n",
    "    # Perform VPG update!\n",
    "    data = buf.get()\n",
    "    piloss,vloss, ent = update(data, pi_optimizer, train_v_iters, vf_optimizer, ac)\n",
    "    pilrun += piloss\n",
    "    vlrun += vloss\n",
    "    entrun += ent\n",
    "    if (epoch % save_freq == 0):\n",
    "        savepth = os.path.join(checkpt,\"agent_\" + str(epoch) + \".pt\")\n",
    "#         savepth += \".pt\"\n",
    "        torch.save(ac.state_dict(), savepth)\n",
    "\n",
    "    if (epoch % log_freq == 0):\n",
    "        piloss_avg = float(pilrun)/float(log_freq)\n",
    "        pilrun = 0\n",
    "        vloss_avg = float(vlrun)/float(log_freq)\n",
    "        vlrun = 0\n",
    "        ent_avg = float(entrun)/float(log_freq)\n",
    "        entrun = 0\n",
    "        \n",
    "        epr_avg = float(epr)/float(log_freq)\n",
    "        epl_avg = float(epl)/float(log_freq)\n",
    "        epr = 0\n",
    "        epl = 0\n",
    "        \n",
    "        writer.add_scalar(f'pi_loss', piloss_avg, epoch)\n",
    "        writer.add_scalar(f'v_loss', vloss_avg, epoch)\n",
    "        \n",
    "        writer.add_scalar(f'ep_length', epl_avg , epoch)\n",
    "        writer.add_scalar(f'reward', epr_avg , epoch)\n",
    "        writer.add_scalar(f'entropy_pi', ent_avg , epoch)\n",
    "        print(f\"{epoch}: rew_aw: {epr_avg} || len_Av: {epl_avg} || pi_loss:{piloss_avg} || v_loss:{vloss_avg} || entropy: {ent_avg}\")\n",
    "        \n",
    "        data = {}\n",
    "        data['pi_loss'] = piloss_avg,\n",
    "        data['v_loss'] = vloss_avg\n",
    "        data['ep_length'] = epl_avg\n",
    "        data['reward'] = epr_avg\n",
    "        data['entropy_pi'] = ent_avg \n",
    "        \n",
    "        logDict[epoch] = data\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "#--logdir=runs/vpg_1\n",
    "# %tensorboard --logdir=runs/vpg_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 29676."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=runs/vpg_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger\n",
    "\n",
    "multi env"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
