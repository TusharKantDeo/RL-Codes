# RL-Codes:: Saring code helpfull for RL Experiments

## Table of Content
- OpenAI-GYM setup : Pong and cartpool

- Generate Experiece:

- Buffer to store trajectories

- Reward Transformations: Advantage, GAE etc

- Normalizing reward

- Observations preprocessing methods

- Models code: Actor, critic etc

- Training Codes

- Optimizers: LR, Initialization etc

- Metrics To track: Reward, entropy , etc

- Tensorboard

- Logging

- Model Checkpoints

- Hyper-parameter search Techniques

- Script to automate process

- Update Rules: Policy Algos

- Actor Update rule: Vanila, TPO, PPO

- Value Funtion fit: Regression on discounted reward, TD(1)

- Documentation and Good Reference

- Referencess :: 1. Spinningup openAI 2. Karapathy Pong from pixel 3. A. https://spinningup.openai.com/en/latest/algorithms/vpg.html https://github.com/openai/spinningup/tree/038665d62d569055401d91856abb287263096178/spinup/algos/pytorch/vpg B. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5

Important Observations

even for episodic task, use GAE or discounted reward for each observation leading upto terminal reward


Note:: Above Code is some collections of code I used for Video suammarization model training and Pong RL Training
